{"cells":[{"cell_type":"code","execution_count":null,"id":"18caabd3-1adb-4edb-bb04-a7a186580555","metadata":{"id":"18caabd3-1adb-4edb-bb04-a7a186580555"},"outputs":[],"source":["!pip install imageio torch matplotlib"]},{"cell_type":"code","execution_count":null,"id":"2d1d9e0d-09d3-4a80-a6a0-488ade4d80dd","metadata":{"id":"2d1d9e0d-09d3-4a80-a6a0-488ade4d80dd"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image, make_grid\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import glob\n","import random\n","import os\n","import math\n","import time\n","from tqdm import tqdm\n","import imageio\n","import itertools\n","import datetime\n","import sys"]},{"cell_type":"code","execution_count":null,"id":"4f755e40-e79f-428d-85b5-3341fee9a7bb","metadata":{"id":"4f755e40-e79f-428d-85b5-3341fee9a7bb"},"outputs":[],"source":["def initialize_weights_normal(module):\n","    class_name = module.__class__.__name__\n","    if class_name.find(\"Conv\") != -1:\n","        torch.nn.init.normal_(module.weight.data, 0.0, 0.02)\n","    elif class_name.find(\"BatchNorm2d\") != -1:\n","        torch.nn.init.normal_(module.weight.data, 1.0, 0.02)\n","        torch.nn.init.constant_(module.bias.data, 0.0)"]},{"cell_type":"code","execution_count":null,"id":"01ed72cc-343a-4376-8992-2ff2e4e86ae4","metadata":{"id":"01ed72cc-343a-4376-8992-2ff2e4e86ae4"},"outputs":[],"source":["class LearningRateScheduler:\n","    def __init__(self, total_epochs, offset_epoch, decay_start_epoch):\n","        assert (total_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n","        self.total_epochs = total_epochs\n","        self.offset_epoch = offset_epoch\n","        self.decay_start_epoch = decay_start_epoch\n","    def step(self, current_epoch):\n","        return 1.0 - max(0, current_epoch + self.offset_epoch - self.decay_start_epoch) / (self.total_epochs - self.decay_start_epoch)"]},{"cell_type":"code","execution_count":null,"id":"c6cce7a0-dca3-451a-a3da-a0e2722ea8d2","metadata":{"id":"c6cce7a0-dca3-451a-a3da-a0e2722ea8d2"},"outputs":[],"source":["class ResidualLayer(nn.Module):\n","    def __init__(self, num_features):\n","        super(ResidualLayer, self).__init__()\n","\n","        convolutional_block = [\n","            nn.ReflectionPad2d(1),\n","            nn.Conv2d(num_features, num_features, 3),\n","            nn.InstanceNorm2d(num_features),\n","            nn.ReLU(inplace=True),\n","            nn.ReflectionPad2d(1),\n","            nn.Conv2d(num_features, num_features, 3),\n","            nn.InstanceNorm2d(num_features),\n","        ]\n","        self.convolutional_block = nn.Sequential(*convolutional_block)\n","    def forward(self, input_tensor):\n","        return input_tensor + self.convolutional_block(input_tensor)"]},{"cell_type":"code","execution_count":null,"id":"af51a3a1-a411-41e4-bc61-c74d8e06a69b","metadata":{"id":"af51a3a1-a411-41e4-bc61-c74d8e06a69b"},"outputs":[],"source":["class FeatureEncoder(nn.Module):\n","    def __init__(self, input_channels=3, initial_dim=64, num_downsamples=2, shared_layer=None):\n","        super(FeatureEncoder, self).__init__()\n","        layers = [\n","            nn.ReflectionPad2d(3),\n","            nn.Conv2d(input_channels, initial_dim, 7),\n","            nn.InstanceNorm2d(initial_dim),\n","            nn.LeakyReLU(0.2, inplace=True),\n","        ]\n","        current_dim = initial_dim\n","        for _ in range(num_downsamples):\n","            layers += [\n","                nn.Conv2d(current_dim, current_dim * 2, 4, stride=2, padding=1),\n","                nn.InstanceNorm2d(current_dim * 2),\n","                nn.ReLU(inplace=True),\n","            ]\n","            current_dim *= 2\n","        for _ in range(3):\n","            layers += [ResidualLayer(current_dim)]\n","        self.model_layers = nn.Sequential(*layers)\n","        self.shared_layer = shared_layer\n","    def reparameterize(self, mean_tensor):\n","        TensorType = torch.cuda.FloatTensor if mean_tensor.is_cuda else torch.FloatTensor\n","        sampled_tensor = Variable(TensorType(np.random.normal(0, 1, mean_tensor.shape)))\n","        return sampled_tensor + mean_tensor\n","    def forward(self, input_data):\n","        encoded_features = self.model_layers(input_data)\n","        mean_tensor = self.shared_layer(encoded_features)\n","        latent_vector = self.reparameterize(mean_tensor)\n","        return mean_tensor, latent_vector"]},{"cell_type":"code","execution_count":null,"id":"b6e452ee-2f5c-401c-b8fb-47307b51e5d1","metadata":{"id":"b6e452ee-2f5c-401c-b8fb-47307b51e5d1"},"outputs":[],"source":["class FeatureGenerator(nn.Module):\n","    def __init__(self, output_channels=3, base_dim=64, num_upsamples=2, shared_layer=None):\n","        super(FeatureGenerator, self).__init__()\n","        self.shared_layer = shared_layer\n","        layers = []\n","        current_dim = base_dim * (2 ** num_upsamples)\n","        for _ in range(3):\n","            layers += [ResidualLayer(current_dim)]\n","        for _ in range(num_upsamples):\n","            layers += [\n","                nn.ConvTranspose2d(current_dim, current_dim // 2, 4, stride=2, padding=1),\n","                nn.InstanceNorm2d(current_dim // 2),\n","                nn.LeakyReLU(0.2, inplace=True),\n","            ]\n","            current_dim //= 2\n","        layers += [\n","            nn.ReflectionPad2d(3),\n","            nn.Conv2d(current_dim, output_channels, 7),\n","            nn.Tanh()\n","        ]\n","        self.model_layers = nn.Sequential(*layers)\n","    def forward(self, input_tensor):\n","        transformed_tensor = self.shared_layer(input_tensor)\n","        generated_output = self.model_layers(transformed_tensor)\n","        return generated_output"]},{"cell_type":"code","execution_count":null,"id":"68746c5b-5e93-403b-8bab-4f3d9d2d18ef","metadata":{"id":"68746c5b-5e93-403b-8bab-4f3d9d2d18ef"},"outputs":[],"source":["class PatchDiscriminator(nn.Module):\n","    def __init__(self, input_shape):\n","        super(PatchDiscriminator, self).__init__()\n","        channels, image_height, image_width = input_shape\n","        self.output_shape = (1, image_height // 2 ** 4, image_width // 2 ** 4)\n","        def create_discriminator_block(input_filters, output_filters, apply_normalization=True):\n","            block_layers = [nn.Conv2d(input_filters, output_filters, 4, stride=2, padding=1)]\n","            if apply_normalization:\n","                block_layers.append(nn.InstanceNorm2d(output_filters))\n","            block_layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return block_layers\n","        self.model_layers = nn.Sequential(\n","            *create_discriminator_block(channels, 64, apply_normalization=False),\n","            *create_discriminator_block(64, 128),\n","            *create_discriminator_block(128, 256),\n","            *create_discriminator_block(256, 512),\n","            nn.Conv2d(512, 1, 3, padding=1)\n","        )\n","    def forward(self, input_image):\n","        return self.model_layers(input_image)"]},{"cell_type":"code","execution_count":null,"id":"f10d9ce6-2860-4010-a03b-9876970123fc","metadata":{"id":"f10d9ce6-2860-4010-a03b-9876970123fc"},"outputs":[],"source":["class PairedImageDataset(Dataset):\n","    def __init__(self, dataset_root, transforms_=None, allow_unaligned=True, dataset_mode=\"train\"):\n","        self.transform = transforms_  # Use the passed Compose object\n","        self.allow_unaligned = allow_unaligned\n","        self.monet_images = sorted(glob.glob(os.path.join(dataset_root, \"monet_jpg\") + \"/*.*\"))\n","        self.photo_images = sorted(glob.glob(os.path.join(dataset_root, \"photo_jpg\") + \"/*.*\"))\n","    def __getitem__(self, index):\n","        monet_image = self.transform(Image.open(self.monet_images[index % len(self.monet_images)]))\n","        if self.allow_unaligned:\n","            photo_image = self.transform(\n","                Image.open(self.photo_images[random.randint(0, len(self.photo_images) - 1)])\n","            )\n","        else:\n","            photo_image = self.transform(Image.open(self.photo_images[index % len(self.photo_images)]))\n","        return {\"monet_image\": monet_image, \"photo_image\": photo_image}\n","    def __len__(self):\n","        return min(len(self.monet_images), len(self.photo_images))"]},{"cell_type":"code","execution_count":null,"id":"b098e11e-a74b-4221-a963-ec75533b6286","metadata":{"id":"b098e11e-a74b-4221-a963-ec75533b6286"},"outputs":[],"source":["is_cuda_available = torch.cuda.is_available()\n","use_cuda = True if is_cuda_available else False"]},{"cell_type":"code","execution_count":null,"id":"988c3c89-5bfd-47d3-96e9-669b1fd7ee8c","metadata":{"id":"988c3c89-5bfd-47d3-96e9-669b1fd7ee8c"},"outputs":[],"source":["output_directory = \"generated_images\"\n","batch_size = 1\n","learning_rate = 0.005\n","beta1 = 0.5\n","beta2 = 0.999\n","learning_rate_decay_epoch = 2000\n","num_workers = 1\n","image_height = 256\n","image_width = 256\n","num_channels = 3\n","sample_save_interval = 2000\n","checkpoint_save_interval = 1000"]},{"cell_type":"code","execution_count":null,"id":"433b65ba-2832-4d3b-a687-1fc48e18257f","metadata":{"id":"433b65ba-2832-4d3b-a687-1fc48e18257f"},"outputs":[],"source":["learning_rates = [0.0002]\n","num_downsampling_layers = 1"]},{"cell_type":"code","execution_count":null,"id":"90704357-b0b4-4f62-af50-34edc41396a5","metadata":{"id":"90704357-b0b4-4f62-af50-34edc41396a5"},"outputs":[],"source":["def denormalize_image(tensor_image, mean_value=0.5, std_value=0.5):\n","    if torch.is_tensor(tensor_image):\n","        tensor_image = tensor_image.detach().numpy()\n","    restored_image = tensor_image * std_value + mean_value\n","    restored_image = restored_image * 255\n","    return np.uint8(restored_image)"]},{"cell_type":"code","execution_count":null,"id":"a92d6d26-348f-4f5d-b3c0-131ba4b94527","metadata":{"id":"a92d6d26-348f-4f5d-b3c0-131ba4b94527"},"outputs":[],"source":["def save_sample_images(batch_number):\n","    images = next(iter(val_dataloader))\n","    monet_images = Variable(images[\"monet_image\"].type(tensor_type))\n","    photo_images = Variable(images[\"photo_image\"].type(tensor_type))\n","    _, monet_latent_vector = encoder_1(monet_images)\n","    _, photo_latent_vector = encoder_2(photo_images)\n","    fake_monet_images = generator_1(photo_latent_vector)\n","    fake_photo_images = generator_2(monet_latent_vector)\n","    monet_grid = denormalize_image(\n","        make_grid(photo_images.cpu(), nrow=4).permute(1, 2, 0).numpy()\n","    )\n","    fake_monet_grid = denormalize_image(\n","        make_grid(fake_monet_images.cpu(), nrow=4).permute(1, 2, 0).numpy()\n","    )\n","    fig, (axis_monet, axis_fake_monet) = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(30, 20))\n","    axis_monet.imshow(monet_grid)\n","    axis_monet.axis(\"off\")\n","    axis_monet.set_title(\"Photo Images (X)\")\n","    axis_fake_monet.imshow(fake_monet_grid)\n","    axis_fake_monet.axis(\"off\")\n","    axis_fake_monet.set_title(\"Generated Monet-like Images (Fake Y)\")\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"67ffb8ad-af39-4394-bac2-29f235ddaebd","metadata":{"id":"67ffb8ad-af39-4394-bac2-29f235ddaebd"},"outputs":[],"source":["def compute_kl_divergence(mean_tensor):\n","    squared_mean = torch.pow(mean_tensor, 2)\n","    kl_loss = torch.mean(squared_mean)\n","    return kl_loss"]},{"cell_type":"code","execution_count":null,"id":"5584508d-5c62-4c0a-8d61-06ca322121b7","metadata":{"id":"5584508d-5c62-4c0a-8d61-06ca322121b7","outputId":"f1f26f4f-1194-4f9d-ffcc-a5b0b9c61fbb"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Epoch 14/15] [Batch 299/300] [D Loss: 0.28558385372161865] [G Loss: 59.693138122558594] ETA: 0:14:39.882703405832311"]}],"source":["base_dim = 64\n","losses_over_epochs = []\n","n_epochs = 15\n","dataset_name = 'gan-getting-started'\n","beta1 = 0.5\n","beta2 = 0.999\n","epoch = 0\n","batches_completed = 0\n","for learning_rate in learning_rates:\n","    gan_loss_criterion = torch.nn.MSELoss()\n","    pixel_loss_criterion = torch.nn.L1Loss()\n","    input_image_shape = (3, 256, 256)\n","    shared_embedding_dim = base_dim * (2 ** num_downsampling_layers)\n","    shared_encoder_block = ResidualLayer(num_features=shared_embedding_dim)\n","    encoder_1 = FeatureEncoder(initial_dim=base_dim, num_downsamples=num_downsampling_layers, shared_layer=shared_encoder_block)\n","    encoder_2 = FeatureEncoder(initial_dim=base_dim, num_downsamples=num_downsampling_layers, shared_layer=shared_encoder_block)\n","    shared_generator_block = ResidualLayer(num_features=shared_embedding_dim)\n","    generator_1 = FeatureGenerator(base_dim=base_dim, num_upsamples=num_downsampling_layers, shared_layer=shared_generator_block)\n","    generator_2 = FeatureGenerator(base_dim=base_dim, num_upsamples=num_downsampling_layers, shared_layer=shared_generator_block)\n","    discriminator_1 = PatchDiscriminator(input_image_shape)\n","    discriminator_2 = PatchDiscriminator(input_image_shape)\n","    if use_cuda:\n","        encoder_1.cuda()\n","        encoder_2.cuda()\n","        generator_1.cuda()\n","        generator_2.cuda()\n","        discriminator_1.cuda()\n","        discriminator_2.cuda()\n","        gan_loss_criterion.cuda()\n","        pixel_loss_criterion.cuda()\n","    for model in [encoder_1, encoder_2, generator_1, generator_2, discriminator_1, discriminator_2]:\n","        model.apply(initialize_weights_normal)\n","    loss_weights = {\n","        \"gan\": 10,\n","        \"kl_encoded\": 0.1,\n","        \"id_pixel\": 100,\n","        \"kl_translated\": 0.1,\n","        \"cycle_pixel\": 100,\n","    }\n","    optimizer_generator = torch.optim.Adam(\n","        itertools.chain(encoder_1.parameters(), encoder_2.parameters(),\n","                        generator_1.parameters(), generator_2.parameters()),\n","        lr=learning_rate,\n","        betas=(beta1, beta2),\n","    )\n","    optimizer_discriminator_1 = torch.optim.Adam(discriminator_1.parameters(), lr=learning_rate, betas=(beta1, beta2))\n","    optimizer_discriminator_2 = torch.optim.Adam(discriminator_2.parameters(), lr=learning_rate, betas=(beta1, beta2))\n","    lr_schedulers = {\n","        \"generator\": torch.optim.lr_scheduler.LambdaLR(\n","            optimizer_generator, lr_lambda=LearningRateScheduler(n_epochs, 0, 0).step\n","        ),\n","        \"discriminator_1\": torch.optim.lr_scheduler.LambdaLR(\n","            optimizer_discriminator_1, lr_lambda=LearningRateScheduler(n_epochs, 0, 0).step\n","        ),\n","        \"discriminator_2\": torch.optim.lr_scheduler.LambdaLR(\n","            optimizer_discriminator_2, lr_lambda=LearningRateScheduler(n_epochs, 0, 0).step\n","        ),\n","    }\n","    tensor_type = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n","    image_transforms = transforms.Compose([\n","        transforms.Resize(int(256 * 1.12), Image.BICUBIC),\n","        transforms.RandomCrop((256, 256)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ])\n","    epoch_losses = []\n","    start_time = time.time()\n","    for epoch in range(n_epochs):\n","        torch.manual_seed(epoch)\n","        epoch_loss_generator = 0\n","        train_loader = DataLoader(\n","        PairedImageDataset(dataset_name, transforms_=image_transforms, allow_unaligned=True),\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=num_workers,\n","        )\n","        val_dataloader = DataLoader(\n","        PairedImageDataset(dataset_name, transforms_=image_transforms, allow_unaligned=True, dataset_mode=\"test\"),\n","        batch_size=5,\n","        shuffle=False,\n","        num_workers=1,\n","        )\n","        for i, batch in enumerate(train_loader):\n","            real_monet_images = Variable(batch[\"monet_image\"].type(tensor_type))\n","            real_photo_images = Variable(batch[\"photo_image\"].type(tensor_type))\n","            valid_labels = Variable(tensor_type(np.ones((real_monet_images.size(0), *discriminator_1.output_shape))),\n","                                    requires_grad=False)\n","            fake_labels = Variable(tensor_type(np.zeros((real_monet_images.size(0), *discriminator_1.output_shape))),\n","                                   requires_grad=False)\n","            optimizer_generator.zero_grad()\n","            monet_mean, monet_latent = encoder_1(real_monet_images)\n","            photo_mean, photo_latent = encoder_2(real_photo_images)\n","            reconstructed_monet = generator_1(monet_latent)\n","            reconstructed_photo = generator_2(photo_latent)\n","            fake_monet = generator_1(photo_latent)\n","            fake_photo = generator_2(monet_latent)\n","            monet_cycle_mean, monet_cycle_latent = encoder_1(fake_monet)\n","            photo_cycle_mean, photo_cycle_latent = encoder_2(fake_photo)\n","            cycle_monet = generator_1(photo_cycle_latent)\n","            cycle_photo = generator_2(monet_cycle_latent)\n","            loss_gan_1 = loss_weights[\"gan\"] * gan_loss_criterion(discriminator_1(fake_monet), valid_labels)\n","            loss_gan_2 = loss_weights[\"gan\"] * gan_loss_criterion(discriminator_2(fake_photo), valid_labels)\n","            loss_kl_monet = loss_weights[\"kl_encoded\"] * compute_kl_divergence(monet_mean)\n","            loss_kl_photo = loss_weights[\"kl_encoded\"] * compute_kl_divergence(photo_mean)\n","            loss_pixel_monet = loss_weights[\"id_pixel\"] * pixel_loss_criterion(reconstructed_monet, real_monet_images)\n","            loss_pixel_photo = loss_weights[\"id_pixel\"] * pixel_loss_criterion(reconstructed_photo, real_photo_images)\n","            loss_cycle_monet = loss_weights[\"cycle_pixel\"] * pixel_loss_criterion(cycle_monet, real_monet_images)\n","            loss_cycle_photo = loss_weights[\"cycle_pixel\"] * pixel_loss_criterion(cycle_photo, real_photo_images)\n","\n","            total_loss_generator = (\n","                loss_gan_1 + loss_gan_2 +\n","                loss_kl_monet + loss_kl_photo +\n","                loss_pixel_monet + loss_pixel_photo +\n","                loss_cycle_monet + loss_cycle_photo\n","            )\n","\n","            total_loss_generator.backward()\n","            optimizer_generator.step()\n","            epoch_loss_generator += total_loss_generator.item() / len(train_loader)\n","            optimizer_discriminator_1.zero_grad()\n","            loss_discriminator_1 = gan_loss_criterion(discriminator_1(real_monet_images), valid_labels) + \\\n","                                   gan_loss_criterion(discriminator_1(fake_monet.detach()), fake_labels)\n","            loss_discriminator_1.backward()\n","            optimizer_discriminator_1.step()\n","            optimizer_discriminator_2.zero_grad()\n","            loss_discriminator_2 = gan_loss_criterion(discriminator_2(real_photo_images), valid_labels) + \\\n","                                   gan_loss_criterion(discriminator_2(fake_photo.detach()), fake_labels)\n","            loss_discriminator_2.backward()\n","            optimizer_discriminator_2.step()\n","            batches_completed = epoch * len(train_loader) + i\n","            estimated_time_left = datetime.timedelta(\n","                seconds=(n_epochs * len(train_loader) - batches_completed) * (time.time() - start_time)\n","            )\n","            sys.stdout.write(\n","                f\"\\r[Epoch {epoch}/{n_epochs}] [Batch {i}/{len(train_loader)}] \"\n","                f\"[D Loss: {(loss_discriminator_1 + loss_discriminator_2).item()}] \"\n","                f\"[G Loss: {total_loss_generator.item()}] ETA: {estimated_time_left}\"\n","            )\n","        if batches_completed % sample_save_interval == 0:\n","            save_sample_images(batches_completed)\n","        epoch_losses.append(epoch_loss_generator)\n","        for scheduler in lr_schedulers.values():\n","            scheduler.step()\n","    losses_over_epochs.append(epoch_losses)"]},{"cell_type":"code","execution_count":null,"id":"8a5e8478-baf8-4c4d-a49f-91956048a214","metadata":{"id":"8a5e8478-baf8-4c4d-a49f-91956048a214","outputId":"95a9c2da-accd-4478-edf8-ee4ce5c19332"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 7038/7038 [06:17<00:00, 18.66it/s]\n"]},{"data":{"text/plain":["FeatureGenerator(\n","  (shared_layer): ResidualLayer(\n","    (convolutional_block): Sequential(\n","      (0): ReflectionPad2d((1, 1, 1, 1))\n","      (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n","      (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (3): ReLU(inplace=True)\n","      (4): ReflectionPad2d((1, 1, 1, 1))\n","      (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n","      (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","    )\n","  )\n","  (model_layers): Sequential(\n","    (0): ResidualLayer(\n","      (convolutional_block): Sequential(\n","        (0): ReflectionPad2d((1, 1, 1, 1))\n","        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n","        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","        (3): ReLU(inplace=True)\n","        (4): ReflectionPad2d((1, 1, 1, 1))\n","        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n","        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      )\n","    )\n","    (1): ResidualLayer(\n","      (convolutional_block): Sequential(\n","        (0): ReflectionPad2d((1, 1, 1, 1))\n","        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n","        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","        (3): ReLU(inplace=True)\n","        (4): ReflectionPad2d((1, 1, 1, 1))\n","        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n","        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      )\n","    )\n","    (2): ResidualLayer(\n","      (convolutional_block): Sequential(\n","        (0): ReflectionPad2d((1, 1, 1, 1))\n","        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n","        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","        (3): ReLU(inplace=True)\n","        (4): ReflectionPad2d((1, 1, 1, 1))\n","        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n","        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      )\n","    )\n","    (3): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (6): ReflectionPad2d((3, 3, 3, 3))\n","    (7): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n","    (8): Tanh()\n","  )\n",")"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","class ImageDataset(Dataset):\n","    def __init__(self, img_path, img_size=256, normalize=True):\n","        self.img_path = img_path\n","        if normalize:\n","            self.transform = transforms.Compose([\n","                transforms.Resize(img_size),\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=[0.5], std=[0.5])\n","            ])\n","        else:\n","            self.transform = transforms.Compose([\n","                transforms.Resize(img_size),\n","                transforms.ToTensor()\n","            ])\n","        self.img_idx = {number_: img_ for number_, img_ in enumerate(os.listdir(self.img_path))}\n","    def __len__(self):\n","        return len(self.img_idx)\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_path, self.img_idx[idx])\n","        img = Image.open(img_path)\n","        img = self.transform(img)\n","        return img\n","encoder_1.eval()\n","generator_1.eval()\n","path_photo = \"gan-getting-started/photo_jpg\"\n","dataset_photo = ImageDataset(path_photo, img_size=256, normalize=True)\n","submit_dataloader = DataLoader(dataset_photo, batch_size=1, shuffle=False)\n","output_dir = \"generatedimages\"\n","os.makedirs(output_dir, exist_ok=True)\n","mean_ = 0.5\n","std_ = 0.5\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","encoder_1.to(device)\n","generator_1.to(device)\n","dataiter = iter(submit_dataloader)\n","for image_idx in tqdm(range(len(submit_dataloader))):\n","    fixed_X = next(dataiter)\n","    _, encod_fake = encoder_1(fixed_X.to(device))\n","    fake_Y = generator_1(encod_fake)\n","    fake_Y = fake_Y.detach().cpu().numpy()\n","    fake_Y = denormalize_image(fake_Y, mean_value=mean_, std_value=std_)\n","    fake_Y = fake_Y[0].transpose(1, 2, 0)\n","    fake_Y = np.uint8(fake_Y)\n","    fake_Y = Image.fromarray(fake_Y)\n","    fake_Y.save(os.path.join(output_dir, f\"{image_idx}.jpg\"))\n","encoder_1.train()\n","generator_1.train()"]},{"cell_type":"code","execution_count":null,"id":"23770441-e4b3-4e51-b6cd-ce7a08d39ed5","metadata":{"id":"23770441-e4b3-4e51-b6cd-ce7a08d39ed5","outputId":"0e377401-fd12-41b5-fceb-05aa586ceb94"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[132.05707829793292, 118.11951235453297, 108.16086034138999, 106.5316303126017, 99.82629423777253, 97.97450312296546, 95.07357529958087, 89.69938290913899, 89.15468419392904, 86.69816014607746, 83.75062998453782, 82.78465667724605, 79.68022748311368, 78.10049790700279, 76.4397444534302]]\n"]}],"source":["print(losses_over_epochs)"]},{"cell_type":"code","execution_count":null,"id":"0c424507-70a4-4f1f-83fd-60a243e09a8d","metadata":{"id":"0c424507-70a4-4f1f-83fd-60a243e09a8d"},"outputs":[],"source":["!pip install pytorch_fid"]},{"cell_type":"code","execution_count":null,"id":"051e9e6e-fd65-41fe-a13a-14cdfe697b8d","metadata":{"id":"051e9e6e-fd65-41fe-a13a-14cdfe697b8d","outputId":"401af29c-4a63-434f-8049-590bf5b01800"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/pt_inception-2015-12-05-6726825d.pth\n","100%|██████████| 91.2M/91.2M [00:01<00:00, 60.8MB/s]\n","100%|██████████| 141/141 [00:49<00:00,  2.83it/s]\n","100%|██████████| 141/141 [00:33<00:00,  4.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["FID Score: 22.316519949675524\n"]}],"source":["from pytorch_fid.fid_score import calculate_fid_given_paths\n","real_images_path = 'gan-getting-started/photo_jpg'\n","generated_images_path = 'generatedimages'\n","fid_score = calculate_fid_given_paths(\n","    paths=[real_images_path, generated_images_path],\n","    batch_size=50,\n","    device='cuda',\n","    dims=2048\n",")\n","print(f\"FID Score: {fid_score}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}